{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e3f11f-9b5c-4f30-afd3-109ce57839a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497fda02-638e-458b-904b-e93d3fcb4c32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    when,\n",
    "    concat_ws,\n",
    "    to_timestamp,\n",
    "    current_timestamp\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c04b1c-0e5c-4aa1-af21-7d2d2aa72664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 0. config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42a460e-39f4-4a6b-8efa-2bd23cb80c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"bronze_container\", \"bronze\")\n",
    "dbutils.widgets.text(\"silver_container\", \"silver\")\n",
    "dbutils.widgets.text(\"sgaccount\", \"sgonpremtoclouddev\")\n",
    "\n",
    "dbutils.widgets.text(\"source_schema\", \"dbo\")\n",
    "dbutils.widgets.text(\"source_table\", \"Orders\")\n",
    "\n",
    "dbutils.widgets.text(\"CATALOG\", \"onpremtocloud_dev_catalog\")\n",
    "dbutils.widgets.text(\"SILVER_SCHEMA\", \"silver\")\n",
    "\n",
    "# Read values passed from ADF\n",
    "bronze_container = dbutils.widgets.get(\"bronze_container\")\n",
    "silver_container = dbutils.widgets.get(\"silver_container\")\n",
    "sgaccount        = dbutils.widgets.get(\"sgaccount\")\n",
    "\n",
    "source_schema    = dbutils.widgets.get(\"source_schema\")\n",
    "source_table     = dbutils.widgets.get(\"source_table\")\n",
    "\n",
    "CATALOG          = dbutils.widgets.get(\"CATALOG\")\n",
    "SILVER_SCHEMA    = dbutils.widgets.get(\"SILVER_SCHEMA\")\n",
    "\n",
    "\n",
    "# bronze_container = \"bronze\"\n",
    "# silver_container = \"silver\"\n",
    "# sgaccount = \"sgonpremtoclouddev\"\n",
    "\n",
    "# source_schema = \"dbo\"\n",
    "# source_table = \"Orders\"\n",
    "\n",
    "# CATALOG = \"onpremtocloud_dev_catalog\"\n",
    "# SILVER_SCHEMA = \"silver\"\n",
    "\n",
    "silver_table_fqn = f\"{CATALOG}.{SILVER_SCHEMA}.{source_table}\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE {SILVER_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efdc159-9170-4cad-9468-e4b69ecf3d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. ADLS path framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c42c8a6-3b28-4d52-853d-1ce7e654ed89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "year = today.strftime(\"%Y\")\n",
    "month = today.strftime(\"%m\")\n",
    "day = today.strftime(\"%d\")\n",
    "\n",
    "# Bronze: daily parquet landing from ADF\n",
    "bronze_path = (\n",
    "    f\"abfss://{bronze_container}@{sgaccount}.dfs.core.windows.net/\"\n",
    "    f\"{year}/{month}/{day}/{source_schema}_{source_table}.parquet\"\n",
    ")\n",
    "print(f\"Bronze path: {bronze_path}\")\n",
    "\n",
    "# Silver: stable Delta table location (no date)\n",
    "silver_path = (\n",
    "    f\"abfss://{silver_container}@{sgaccount}.dfs.core.windows.net/{source_table}\"\n",
    ")\n",
    "print(f\"Silver path: {silver_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9246a80e-6e1a-4a1f-a537-26944be02c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Read raw customers from bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d4c104-10eb-41a9-92d3-930d2be7dda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bz_orders = spark.read.format(\"parquet\").load(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d16cd1-74b3-4927-afa1-7977c644d498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Basic transformations & SCD columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9399dc60-847f-4b98-8314-babd8b2b76ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_ts = current_timestamp()  # one timestamp per run\n",
    "\n",
    "df_stage = (\n",
    "    df_bz_orders\n",
    "    # Deduplicate by business key (adjust if composite key)\n",
    "    .dropDuplicates([\"OrderID\"])\n",
    "    # Core business columns\n",
    "    .fillna({\"OrderStatus\":\"Unknown\"})\n",
    "    .withColumn(\"OrderDate\", to_timestamp(\"OrderDate\"))\n",
    "    .withColumn(\"CreatedDate\", to_timestamp(\"CreatedDate\"))\n",
    "    .withColumn(\"ModifiedDate\", to_timestamp(\"ModifiedDate\"))\n",
    "    # SCD2 columns\n",
    "    .withColumn(\"LoadDate\", load_ts)\n",
    "    .withColumn(\"StartDate\", load_ts)\n",
    "    .withColumn(\"EndDate\", lit(None).cast(\"timestamp\"))\n",
    "    .withColumn(\"IsCurrent\", lit(True))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea87da25-99c6-4c8b-8d59-e3293de7d40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. Check if silver table already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6535650f-943d-49b0-8ef9-a0a8c6c2ef6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    silver_df = spark.table(silver_table_fqn)\n",
    "    table_exists = True\n",
    "    silver_schema = silver_df.schema\n",
    "    print(f\"Silver table exists: {silver_table_fqn}\")\n",
    "except AnalysisException:\n",
    "    table_exists = False\n",
    "    silver_schema = None\n",
    "    print(f\"Silver table DOES NOT exist yet: {silver_table_fqn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d23f9c-4f0e-48e1-991a-d03824823acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6. Insert data into silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f75b53b-1971-4801-b9e4-3e502439bd93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists:\n",
    "    # First run: create Delta + UC table\n",
    "    print(\"First load: creating Delta at silver_path and registering UC table...\")\n",
    "\n",
    "    # Write initial full snapshot as Delta\n",
    "    (\n",
    "        df_stage\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(silver_path)\n",
    "    )\n",
    "\n",
    "    # Register as external table in Unity Catalog\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {silver_table_fqn}\n",
    "        USING DELTA\n",
    "        LOCATION '{silver_path}'\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Initial silver table created: {silver_table_fqn}\")\n",
    "\n",
    "else:\n",
    "    # Subsequent runs: handle schema drift & SCD2 merge\n",
    "    print(\"Incremental load: aligning schema with existing silver and applying SCD2 MERGE...\")\n",
    "\n",
    "    # 6.1 Align incoming data (df_stage) to existing silver schema (dynamic handling)\n",
    "    df = df_stage\n",
    "\n",
    "    # Add any missing columns as nulls, cast existing to expected types\n",
    "    \n",
    "    for field in silver_schema:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "\n",
    "    # Keep only columns that exist in silver, in correct order\n",
    "    aligned_cols = [f.name for f in silver_schema]\n",
    "    updates_df = df.select(aligned_cols).alias(\"updates\")\n",
    "\n",
    "    # 6.2 Load DeltaTable for silver\n",
    "    delta_orders = DeltaTable.forPath(spark, silver_path)\n",
    "\n",
    "    # SCD2 step 1: close changed current records\n",
    "    (\n",
    "        delta_orders.alias(\"existing\")\n",
    "        .merge(\n",
    "            updates_df,\n",
    "            \"\"\"           \n",
    "            existing.OrderID = updates.OrderID \n",
    "            AND existing.IsCurrent = true\n",
    "            AND (\n",
    "                existing.CustomerID <> updates.CustomerID OR\n",
    "                existing.OrderDate <> updates.OrderDate OR\n",
    "                existing.OrderStatus <> updates.OrderStatus\n",
    "            )\n",
    "            \"\"\"\n",
    "        )\n",
    "        .whenMatchedUpdate(\n",
    "            set={\n",
    "                \"EndDate\":   \"updates.LoadDate\",\n",
    "                \"IsCurrent\": \"false\"\n",
    "            }\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    # SCD2 step 2: insert new current records (new customers OR changed ones after closing old row)\n",
    "    insert_values = {c: f\"updates.{c}\" for c in aligned_cols}\n",
    "\n",
    "    (\n",
    "        delta_orders.alias(\"existing\")\n",
    "        .merge(\n",
    "            updates_df,\n",
    "            \"\"\"\n",
    "            existing.OrderID = updates.OrderID\n",
    "            AND existing.IsCurrent = true\n",
    "            \"\"\"\n",
    "        )\n",
    "        .whenNotMatchedInsert(values=insert_values)\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    print(\"SCD2 merge for silver.Orders completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4c67d4-e9c9-4d82-b0f0-f839f01d92d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7432337097887438,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Silver_OnpremToCloud_Orders",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
